{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb24145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf255868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Organization in the Organization.csv: 392443\n",
      "Number of columns in the Organization.csv: 41\n",
      "Column names in the Organization.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "legal_name\n",
      "roles\n",
      "domain\n",
      "homepage_url\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "address\n",
      "postal_code\n",
      "status\n",
      "short_description\n",
      "category_list\n",
      "category_groups_list\n",
      "num_funding_rounds\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "last_funding_on\n",
      "closed_on\n",
      "employee_count\n",
      "email\n",
      "phone\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "logo_url\n",
      "alias1\n",
      "alias2\n",
      "alias3\n",
      "primary_role\n",
      "num_exits\n"
     ]
    }
   ],
   "source": [
    "# organization_path = '../Dataset/bulk_export/organizations.csv' \n",
    "organization_path = '../Dataset/filtered_data/organizations_filtered_by_date_and_roles.csv' \n",
    "organization_data = pd.read_csv(organization_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(organization_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(organization_data.describe())\n",
    "\n",
    "organization_data_num_rows = organization_data.shape[0]\n",
    "print(\"Number of Organization in the Organization.csv:\", organization_data_num_rows)\n",
    "organization_data_num_columns = organization_data.shape[1] \n",
    "print(\"Number of columns in the Organization.csv:\", organization_data_num_columns)\n",
    "organization_data_column_names = organization_data.columns\n",
    "print(\"Column names in the Organization.csv:\")\n",
    "for name in organization_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f861bfd",
   "metadata": {},
   "source": [
    "# Preprocessing the Data \n",
    "\n",
    "## Filtering the companies\n",
    "\n",
    "1 - I filtered companies using their creation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85901b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define the warm up window \n",
    "start_date = datetime.strptime('2015-01-01', '%Y-%m-%d') # Start Day: 1 of January of 2015\n",
    "end_date = datetime.strptime('2018-12-31', '%Y-%m-%d') # End Day: 31 of December of 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30d2c52",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset/bulk_export/organizations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Dataset/bulk_export/organizations.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Dataset/filtered_data/organizations_filtered_by_date.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n\u001b[1;32m      4\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[1;32m      5\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(f_out)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dataset/bulk_export/organizations.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('../Dataset/bulk_export/organizations.csv', 'r') as f, open('../Dataset/filtered_data/organizations_filtered_by_date.csv', 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(f_out)\n",
    "\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    date_index = header.index('founded_on')\n",
    "\n",
    "    # Read the rest of the csv and write rows where the date is within the range\n",
    "    for row in reader:\n",
    "        date_str = row[date_index]\n",
    "        if date_str:  # Check if the date string is not empty\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                if start_date <= date <= end_date:\n",
    "                    writer.writerow(row)\n",
    "            except ValueError:\n",
    "                print(\"Error: Unable to parse date string '{}'\".format(date_str))\n",
    "        else:\n",
    "            print(\"Warning: Empty date string encountered in row {}\".format(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e29d8",
   "metadata": {},
   "source": [
    "Looking at the dataset, we see that there is a column that also let us differentiate between an Companies and Investment firms in the domain column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbe5da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset/filtered_data/organizations_filtered_by_date.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Dataset/filtered_data/organizations_filtered_by_date.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Dataset/filtered_data/organizations_filtered_by_date_and_roles.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n\u001b[1;32m      2\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[1;32m      3\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(f_out)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dataset/filtered_data/organizations_filtered_by_date.csv'"
     ]
    }
   ],
   "source": [
    "with open('../Dataset/filtered_data/organizations_filtered_by_date.csv', 'r') as f, open('../Dataset/filtered_data/organizations_filtered_by_date_and_roles.csv', 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(f_out)\n",
    "\n",
    "    header = next(reader)  # This gets the header row\n",
    "    writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "    # Find the index of the date column (replace 'date_column' with your column's name)\n",
    "    roles_index = header.index('roles')\n",
    "\n",
    "    # Read the rest of the csv and write rows where the date is within the range\n",
    "    for row in reader:\n",
    "        roles_string = row[roles_index]\n",
    "        if roles_string:  # Check if the date string is not empty\n",
    "            try:\n",
    "                if roles_string == \"company\":\n",
    "                    writer.writerow(row)\n",
    "            except ValueError:\n",
    "                print(\"Error: Unable to parse string string '{}'\".format(roles_string))\n",
    "        else:\n",
    "            print(\"Warning: Empty string string encountered in row {}\".format(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd53bf4",
   "metadata": {},
   "source": [
    "Clean the database by dropping the columns that are not relevant for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa14662",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'organization_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m      2\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcb_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_role\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_exits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Drop the specified columns\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m organizations_data \u001b[38;5;241m=\u001b[39m organization_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcolumns_to_drop)\n\u001b[1;32m     26\u001b[0m organizations_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Dataset/filtered_data/filtered_organization.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'organization_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the columns to drop\n",
    "columns_to_drop = ['type', \n",
    "    'cb_url', \n",
    "    'rank', \n",
    "    'created_at', \n",
    "    'updated_at', \n",
    "    'legal_name', \n",
    "    'roles', \n",
    "    'cb_url', \n",
    "    'rank', \n",
    "    'created_at', \n",
    "    'updated_at', \n",
    "    'legal_name', \n",
    "    'roles', \n",
    "    'email', \n",
    "    'phone', \n",
    "    'logo_url', \n",
    "    'alias1', \n",
    "    'alias2', \n",
    "    'alias3', \n",
    "    'primary_role', \n",
    "    'num_exits']\n",
    "\n",
    "# Drop the specified columns\n",
    "organizations_data = organization_data.drop(columns=columns_to_drop)\n",
    "organizations_data.to_csv('../Dataset/filtered_data/filtered_organization.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59944f9e",
   "metadata": {},
   "source": [
    "### Now I merge the organizations with their funding data, IPO data, closure data to further filtering by IPO/Acquisition/Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48fb0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset/bulk_export/funding_rounds.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m funding_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Dataset/bulk_export/funding_rounds.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m----> 3\u001b[0m funding_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(funding_path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dataset/bulk_export/funding_rounds.csv'"
     ]
    }
   ],
   "source": [
    "funding_path = '../Dataset/bulk_export/funding_rounds.csv' \n",
    "\n",
    "funding_data = pd.read_csv(funding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05904bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_path = '../Dataset/filtered_data/filtered_organization.csv'\n",
    "\n",
    "organizations_data = pd.read_csv(organizations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346511c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = organizations_data.merge(funding_data, left_on='uuid', right_on='org_uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../Dataset/filtered_data/filtered_organization_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_path = '../Dataset/filtered_data/filtered_organization_merged.csv' \n",
    "\n",
    "merged_df = pd.read_csv(merged_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c1540",
   "metadata": {},
   "source": [
    "First - I removed the ones closed during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['closed_on'] = pd.to_datetime(merged_df['closed_on'])\n",
    "filtered_close_merged_df = merged_df[(merged_df['closed_on'] < start_date) | (merged_df['closed_on'] > end_date) | (merged_df['closed_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec9fb2",
   "metadata": {},
   "source": [
    "Now I removed the ones acquired during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42754ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of acquisitions in the acquisitions.csv: 162216\n",
      "Number of columns in the acquisitions.csv: 27\n",
      "Column names in the acquisitions.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "acquiree_uuid\n",
      "acquiree_name\n",
      "acquiree_cb_url\n",
      "acquiree_country_code\n",
      "acquiree_state_code\n",
      "acquiree_region\n",
      "acquiree_city\n",
      "acquirer_uuid\n",
      "acquirer_name\n",
      "acquirer_cb_url\n",
      "acquirer_country_code\n",
      "acquirer_state_code\n",
      "acquirer_region\n",
      "acquirer_city\n",
      "acquisition_type\n",
      "acquired_on\n",
      "price_usd\n",
      "price\n",
      "price_currency_code\n"
     ]
    }
   ],
   "source": [
    "acquisitions_path = '../Dataset/bulk_export/acquisitions.csv'\n",
    "acquisitions_data = pd.read_csv(acquisitions_path)\n",
    "acquisitions_data_num_rows = acquisitions_data.shape[0] \n",
    "print(\"Number of acquisitions in the acquisitions.csv:\", acquisitions_data_num_rows)\n",
    "acquisitions_data_num_columns = acquisitions_data.shape[1] \n",
    "print(\"Number of columns in the acquisitions.csv:\", acquisitions_data_num_columns)\n",
    "acquisitions_data_column_names = acquisitions_data.columns\n",
    "print(\"Column names in the acquisitions.csv:\")\n",
    "for name in acquisitions_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df =  filtered_close_merged_df.merge(acquisitions_data, left_on='uuid_x', right_on='acquiree_uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df.to_csv('../Dataset/filtered_data/acquired_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df['acquired_on'] = pd.to_datetime(acquired_df['acquired_on'])\n",
    "filtered_close_acquired_df = acquired_df[(acquired_df['acquired_on'] < start_date) | (acquired_df['acquired_on'] > end_date) | (acquired_df['acquired_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acf60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_close_acquired_df.to_csv('../Dataset/filtered_data/filtered_close_acquired_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521219e",
   "metadata": {},
   "source": [
    "Now I removed the ones that IPO during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ipos in the ipos.csv: 50368\n",
      "Number of columns in the ipos.csv: 27\n",
      "Column names in the ipos.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "org_uuid\n",
      "org_name\n",
      "org_cb_url\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "stock_exchange_symbol\n",
      "stock_symbol\n",
      "went_public_on\n",
      "share_price_usd\n",
      "share_price\n",
      "share_price_currency_code\n",
      "valuation_price_usd\n",
      "valuation_price\n",
      "valuation_price_currency_code\n",
      "money_raised_usd\n",
      "money_raised\n",
      "money_raised_currency_code\n"
     ]
    }
   ],
   "source": [
    "ipos_path = '../Dataset/bulk_export/ipos.csv'  # Replace 'your_file.csv' with the actual path to your CSV file\n",
    "ipos_data = pd.read_csv(ipos_path)\n",
    "ipos_data_num_rows = ipos_data.shape[0] \n",
    "print(\"Number of ipos in the ipos.csv:\", ipos_data_num_rows)\n",
    "ipos_data_num_columns = ipos_data.shape[1] \n",
    "print(\"Number of columns in the ipos.csv:\", ipos_data_num_columns)\n",
    "ipos_data_column_names = ipos_data.columns\n",
    "print(\"Column names in the ipos.csv:\")\n",
    "for name in ipos_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns before merging to ensure they have unique names\n",
    "filtered_close_acquired_df.rename(columns={'uuid_x': 'uuid_org'}, inplace=True)\n",
    "filtered_close_acquired_df.rename(columns={'name_x': 'name_org'}, inplace=True)\n",
    "filtered_close_acquired_df.rename(columns={'permalink_x': 'permalink_org'}, inplace=True)\n",
    "\n",
    "ipos_data.rename(columns={'org_uuid': 'uuid_ipos'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00920a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_df =  filtered_close_acquired_df.merge(ipos_data, left_on='uuid_org', right_on='uuid_ipos', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a225d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_df.to_csv('../Dataset/filtered_data/ipos_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd370692",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_df['went_public_on'] = pd.to_datetime(ipos_df['went_public_on'])\n",
    "filtered_close_acquired_ipos_df = ipos_df[(ipos_df['went_public_on'] < start_date) | (ipos_df['went_public_on'] > end_date) | (ipos_df['went_public_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_close_acquired_ipos_df.to_csv('../Dataset/filtered_data/filtered_close_acquired_ipos_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bafcc26",
   "metadata": {},
   "source": [
    "## I filter the companies which closed a funding round above Series B or above, which is not interesting for Plug and Play investments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the series that are Series B or above\n",
    "series_b_or_above = ['series_b', 'series_c', 'series_d', 'series_e', 'series_f', 'series_g']\n",
    "\n",
    "# Filter the DataFrame to exclude organizations that have a series B or above in their investment_type\n",
    "\n",
    "organizations_to_remove = filtered_close_acquired_ipos_df[filtered_close_acquired_ipos_df['investment_type'].isin(series_b_or_above)]['org_uuid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series_df = filtered_close_acquired_ipos_df[~filtered_close_acquired_ipos_df['org_uuid'].isin(organizations_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series_df.to_csv('../Dataset/filtered_data/filtered_series_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5eaed7",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_path = '../Dataset/filtered_data/filtered_series_df.csv'\n",
    "\n",
    "filtered_data = pd.read_csv(filtered_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8537d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some companies are duplicate because they had a few rounds, this is for taking them away\n",
    "unique_filtered = filtered_data.drop_duplicates(subset=['uuid_org'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = unique_filtered.shape[0]\n",
    "\n",
    "print(f\"There is {num_rows} companies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fa86e",
   "metadata": {},
   "source": [
    "Now we need to clean the data set by removing unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = unique_filtered.columns.tolist()\n",
    "\n",
    "# Join the column names with commas\n",
    "column_names_str = '\\', \\''.join(column_names)\n",
    "\n",
    "# Print the column names\n",
    "print(column_names_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11290f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['uuid_y', 'name_y', 'type_x', 'permalink_y', 'cb_url_x', 'rank_x', 'created_at_x', 'updated_at_x', 'country_code_y', 'state_code_y', 'region_y', 'city_y', 'investment_type', 'announced_on', 'raised_amount_usd', 'raised_amount', 'raised_amount_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_valuation_currency_code', 'investor_count', 'org_uuid', 'org_name_x', 'lead_investor_uuids', 'uuid_x', 'name_x', 'type_y', 'permalink_x', 'cb_url_y', 'rank_y', 'created_at_y', 'updated_at_y', 'acquiree_uuid', 'acquiree_name', 'acquiree_cb_url', 'acquiree_country_code', 'acquiree_state_code', 'acquiree_region', 'acquiree_city', 'acquirer_uuid', 'acquirer_name', 'acquirer_cb_url', 'acquirer_country_code', 'acquirer_state_code', 'acquirer_region', 'acquirer_city', 'acquisition_type', 'acquired_on', 'price_usd', 'price', 'price_currency_code', 'uuid_y.1', 'name_y.1', 'type', 'permalink_y.1', 'cb_url', 'rank', 'created_at', 'updated_at', 'uuid_ipos', 'org_name_y', 'org_cb_url', 'country_code', 'state_code', 'region', 'city', 'stock_exchange_symbol', 'stock_symbol', 'went_public_on', 'share_price_usd', 'share_price', 'share_price_currency_code', 'valuation_price_usd', 'valuation_price', 'valuation_price_currency_code', 'money_raised_usd', 'money_raised', 'money_raised_currency_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered = unique_filtered.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1d5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uuid_org', 'name_org', 'permalink_org', 'domain', 'homepage_url', 'country_code_x', 'state_code_x', 'region_x', 'city_x', 'address', 'postal_code', 'status', 'short_description', 'category_list', 'category_groups_list', 'num_funding_rounds', 'total_funding_usd', 'total_funding', 'total_funding_currency_code', 'founded_on', 'last_funding_on', 'closed_on', 'employee_count', 'facebook_url', 'linkedin_url', 'twitter_url\n"
     ]
    }
   ],
   "source": [
    "column_names = unique_filtered.columns.tolist()\n",
    "\n",
    "# Join the column names with commas\n",
    "column_names_str = '\\', \\''.join(column_names)\n",
    "\n",
    "# Print the column names\n",
    "print(column_names_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facda5bd",
   "metadata": {},
   "source": [
    "# Now we need to add the predictor variables \n",
    "\n",
    "And also delete some attributes that I do not consider relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc885940",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.rename(columns={'country_code_x': 'country_code'}, inplace=True)\n",
    "unique_filtered.rename(columns={'state_code_x': 'state_code'}, inplace=True)\n",
    "unique_filtered.rename(columns={'region_x': 'region'}, inplace=True)\n",
    "unique_filtered.rename(columns={'city_x': 'city'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('../Dataset/filtered_data/unique_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400546d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered = unique_filtered.drop(columns='employee_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf63205",
   "metadata": {},
   "source": [
    "Nota para alde: creo que deberia quitar las compañias que tengan al menos 1 funding round pero no diga la cantidad... No lo se\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc767f38",
   "metadata": {},
   "source": [
    "## Company Information\n",
    "\n",
    "we included the company age in months (age_months) at the beginning of the simulation (ts) and we added variables that measure the presence of the company in social media networks. \n",
    "\n",
    "These binary variables indicate whether the company registered in Crunchbase its con- tact information (has_email and has_phone) or a Facebook (has_facebook_url), Twitter (has_twitter_url) or Linkedin (has_linkedin_url) account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning of the simulation period\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the warm up window \n",
    "simulation_start_date = datetime.strptime('2019-01-01', '%Y-%m-%d') # Start Day: 1 of January of 2019\n",
    "simulation_end_date = datetime.strptime('2022-12-31', '%Y-%m-%d') # End Day: 31 of December of 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e2cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m unique_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfounded_on\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43munique_filtered\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfounded_on\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m unique_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_months\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ((simulation_start_date \u001b[38;5;241m-\u001b[39m unique_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfounded_on\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdays \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(math\u001b[38;5;241m.\u001b[39mceil)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "unique_filtered['founded_on'] = pd.to_datetime(unique_filtered['founded_on'])\n",
    "\n",
    "unique_filtered['age_months'] = ((simulation_start_date - unique_filtered['founded_on']).dt.days / 30).apply(math.ceil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if the URL is present\n",
    "def has_url(url):\n",
    "    return 1 if pd.notnull(url) else 0\n",
    "\n",
    "# Convert URLs into binary variables\n",
    "unique_filtered['has_facebook_url'] = unique_filtered['facebook_url'].apply(has_url)\n",
    "unique_filtered['has_twitter_url'] = unique_filtered['twitter_url'].apply(has_url)\n",
    "unique_filtered['has_linkedin_url'] = unique_filtered['linkedin_url'].apply(has_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('../Dataset/filtered_data/unique_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13e1d4",
   "metadata": {
    "tags": [
     "Funding Filtering"
    ]
   },
   "source": [
    "## Funding information\n",
    "\n",
    "The variables in this category summarize the funding events occurred in a company during the Warmup window. The availability of a temporal series of funding round events in Crunchbase allowed us to synthesize information about:\n",
    "\n",
    "- Number of funding rounds that the company achieved before ts (round_count) and the total amount raised in those rounds(raised_amount_usd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce9274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of funding_rounds in the funding_rounds.csv: 631092\n",
      "Number of columns in the funding_rounds.csv: 24\n",
      "Column names in the funding_rounds.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "investment_type\n",
      "announced_on\n",
      "raised_amount_usd\n",
      "raised_amount\n",
      "raised_amount_currency_code\n",
      "post_money_valuation_usd\n",
      "post_money_valuation\n",
      "post_money_valuation_currency_code\n",
      "investor_count\n",
      "org_uuid\n",
      "org_name\n",
      "lead_investor_uuids\n"
     ]
    }
   ],
   "source": [
    "funding_rounds_path = '../Dataset/bulk_export/funding_rounds.csv' \n",
    "funding_rounds_data = pd.read_csv(funding_rounds_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(funding_rounds_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(funding_rounds_data.describe())\n",
    "\n",
    "funding_rounds_data_num_rows = funding_rounds_data.shape[0]\n",
    "print(\"Number of funding_rounds in the funding_rounds.csv:\", funding_rounds_data_num_rows)\n",
    "funding_rounds_data_num_columns = funding_rounds_data.shape[1] \n",
    "print(\"Number of columns in the funding_rounds.csv:\", funding_rounds_data_num_columns)\n",
    "funding_rounds_data_column_names = funding_rounds_data.columns\n",
    "print(\"Column names in the funding_rounds.csv:\")\n",
    "for name in funding_rounds_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783891ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the unique_filtered.csv:\n",
      "uuid_org\n",
      "name_org\n",
      "permalink_org\n",
      "domain\n",
      "homepage_url\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "address\n",
      "postal_code\n",
      "status\n",
      "short_description\n",
      "category_list\n",
      "category_groups_list\n",
      "num_funding_rounds\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "last_funding_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "age_months\n",
      "has_facebook_url\n",
      "has_twitter_url\n",
      "has_linkedin_url\n"
     ]
    }
   ],
   "source": [
    "unique_filtered = pd.read_csv('../Dataset/filtered_data/unique_filtered.csv')\n",
    "unique_filtered_column_names = unique_filtered.columns\n",
    "print(\"Column names in the unique_filtered.csv:\")\n",
    "for name in unique_filtered_column_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_7300/852572325.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  funding_rounds['announced_on'] = pd.to_datetime(funding_rounds['announced_on'])\n",
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_7300/852572325.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  unique_filtered['round_count'].fillna(0, inplace=True)\n",
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_7300/852572325.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  unique_filtered['raised_amount_usd'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Convert relevant date columns to datetime format\n",
    "unique_filtered['founded_on'] = pd.to_datetime(unique_filtered['founded_on'])\n",
    "unique_filtered['last_funding_on'] = pd.to_datetime(unique_filtered['last_funding_on'])\n",
    "\n",
    "funding_rounds = funding_rounds_data[funding_rounds_data['org_uuid'].isin(unique_filtered['uuid_org'])]\n",
    "\n",
    "funding_rounds['announced_on'] = pd.to_datetime(funding_rounds['announced_on'])\n",
    "\n",
    "# Set the simulation start date (ts) to January 1, 2019\n",
    "ts = pd.to_datetime('2019-01-01')\n",
    "\n",
    "# 3. Filter the funding rounds that occurred before ts\n",
    "funding_before_ts = funding_rounds[funding_rounds['announced_on'] < ts]\n",
    "\n",
    "# 4. Count the number of funding rounds for each company\n",
    "round_count = funding_before_ts.groupby('org_uuid').size().reset_index(name='round_count')\n",
    "\n",
    "# 5. Sum up the raised amount in those rounds for each company\n",
    "raised_amount_usd = funding_before_ts.groupby('org_uuid')['raised_amount_usd'].sum().reset_index(name='raised_amount_usd')\n",
    "\n",
    "# Merge round count and raised amount into unique_filtered DataFrame\n",
    "unique_filtered = unique_filtered.merge(round_count, left_on=\"uuid_org\", right_on='org_uuid', how='left', suffixes=('', '_x'))\n",
    "unique_filtered = unique_filtered.merge(raised_amount_usd, left_on=\"uuid_org\", right_on='org_uuid', how='left', suffixes=('', '_y'))\n",
    "\n",
    "# Replace NaN values with 0 for companies with no data\n",
    "unique_filtered['round_count'].fillna(0, inplace=True)\n",
    "unique_filtered['raised_amount_usd'].fillna(0, inplace=True)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "unique_filtered.to_csv('unique_filtered_with_funding_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bc2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the unique_filtered.csv:\n",
      "uuid_org\n",
      "name_org\n",
      "permalink_org\n",
      "domain\n",
      "homepage_url\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "address\n",
      "postal_code\n",
      "status\n",
      "short_description\n",
      "category_list\n",
      "category_groups_list\n",
      "num_funding_rounds\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "last_funding_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "age_months\n",
      "has_facebook_url\n",
      "has_twitter_url\n",
      "has_linkedin_url\n",
      "round_count\n",
      "raised_amount_usd\n",
      "last_round_investment_type\n",
      "last_round_raised_amount_usd\n",
      "last_round_post_money_valuation\n",
      "last_round_timelapse_months\n"
     ]
    }
   ],
   "source": [
    "unique_filtered_column_names = unique_filtered.columns\n",
    "print(\"Column names in the unique_filtered.csv:\")\n",
    "for name in unique_filtered_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered = pd.read_csv('unique_filtered_with_funding_info.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered = unique_filtered.drop(columns=['org_uuid', 'org_uuid_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b78dd7",
   "metadata": {},
   "source": [
    "- Data about the last funding round in Warmup window: \n",
    "\n",
    "• funding round type (last_round_investment_type)\n",
    "\n",
    "• amount raised(last_round_raised_amount_usd)\n",
    "\n",
    "• company valuation after this funding round(last_round_post_money_valuation\n",
    "\n",
    "• time lapsed, in months, between the beginning of the simulation (ts) and when this funding round occurred(last_round_timelapse_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a29b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 2. Find the last funding round for each company within the Warmup window\n",
    "last_round_warmup = funding_before_ts.groupby('org_uuid').last()\n",
    "\n",
    "# 3. Extract the required data from the last funding round for each company\n",
    "last_round_investment_type = last_round_warmup['investment_type'].rename('last_round_investment_type')\n",
    "last_round_raised_amount_usd = last_round_warmup['raised_amount_usd'].rename('last_round_raised_amount_usd')\n",
    "last_round_post_money_valuation = last_round_warmup['post_money_valuation_usd'].rename('last_round_post_money_valuation')\n",
    "# 4. Calculate the time lapse in months between simulation start date and the last funding round\n",
    "last_round_timelapse_months = ((simulation_start_date - last_round_warmup['announced_on']).dt.days / 30).apply(math.ceil).astype(int).rename('last_round_timelapse_months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77989038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extracted data to merged_unique_funding_df DataFrame\n",
    "#unique_filtered['last_round_investment_type'] = last_round_investment_type\n",
    "#unique_filtered['last_round_raised_amount_usd'] = last_round_raised_amount_usd\n",
    "#unique_filtered['last_round_post_money_valuation'] = last_round_post_money_valuation\n",
    "#unique_filtered['last_round_timelapse_months'] = last_round_timelapse_months\n",
    "\n",
    "unique_filtered = unique_filtered.merge(last_round_investment_type, left_on=\"uuid_org\", right_on='org_uuid', how='left')\n",
    "unique_filtered = unique_filtered.merge(last_round_raised_amount_usd, left_on=\"uuid_org\", right_on='org_uuid', how='left')\n",
    "unique_filtered = unique_filtered.merge(last_round_post_money_valuation, left_on=\"uuid_org\", right_on='org_uuid', how='left')\n",
    "unique_filtered = unique_filtered.merge(last_round_timelapse_months, left_on=\"uuid_org\", right_on='org_uuid', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131986b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('unique_filtered_with_funding_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd49182",
   "metadata": {},
   "source": [
    "- Number of (unique) investors who participated in the funding rounds during the Warmup window (investor_count) and specifically, in the last funding (last_round_investor_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c56cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of investments in the investments.csv: 1027342\n",
      "Number of columns in the investments.csv: 14\n",
      "Column names in the investments.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "funding_round_uuid\n",
      "funding_round_name\n",
      "investor_uuid\n",
      "investor_name\n",
      "investor_type\n",
      "is_lead_investor\n"
     ]
    }
   ],
   "source": [
    "investments_path = '../Dataset/bulk_export/investments.csv' \n",
    "investments_data = pd.read_csv(investments_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(investments_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(investments_data.describe())\n",
    "\n",
    "investments_data_num_rows = investments_data.shape[0]\n",
    "print(\"Number of investments in the investments.csv:\", investments_data_num_rows)\n",
    "investments_data_num_columns = investments_data.shape[1] \n",
    "print(\"Number of columns in the investments.csv:\", investments_data_num_columns)\n",
    "investments_data_column_names = investments_data.columns\n",
    "print(\"Column names in the investments.csv:\")\n",
    "for name in investments_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731276b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of investors in the investors.csv: 274366\n",
      "Number of columns in the investors.csv: 25\n",
      "Column names in the investors.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "roles\n",
      "domain\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "investor_types\n",
      "investment_count\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "logo_url\n"
     ]
    }
   ],
   "source": [
    "investors_path = '../Dataset/bulk_export/investors.csv' \n",
    "investors_data = pd.read_csv(investors_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(investors_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(investors_data.describe())\n",
    "\n",
    "investors_data_num_rows = investors_data.shape[0]\n",
    "print(\"Number of investors in the investors.csv:\", investors_data_num_rows)\n",
    "investors_data_num_columns = investors_data.shape[1] \n",
    "print(\"Number of columns in the investors.csv:\", investors_data_num_columns)\n",
    "investors_data_column_names = investors_data.columns\n",
    "print(\"Column names in the investors.csv:\")\n",
    "for name in investors_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f7e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the funding_before_ts.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "investment_type\n",
      "announced_on\n",
      "raised_amount_usd\n",
      "raised_amount\n",
      "raised_amount_currency_code\n",
      "post_money_valuation_usd\n",
      "post_money_valuation\n",
      "post_money_valuation_currency_code\n",
      "investor_count\n",
      "org_uuid\n",
      "org_name\n",
      "lead_investor_uuids\n",
      "investor_countwup\n",
      "last_round_investor_count\n"
     ]
    }
   ],
   "source": [
    "funding_before_ts_column_names = funding_before_ts.columns\n",
    "print(\"Column names in the funding_before_ts.csv:\")\n",
    "for name in funding_before_ts_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter the investments that occurred during the Warmup window\n",
    "investments_warmup = investments_data[investments_data['funding_round_uuid'].isin(funding_before_ts['uuid'])]\n",
    "\n",
    "# 2. Calculate the number of unique investors for each company during the Warmup window\n",
    "investor_count = investments_warmup.groupby('funding_round_uuid')['investor_uuid'].nunique().rename('investor_count')\n",
    "\n",
    "# Merge investor count into funding_rounds DataFrame\n",
    "funding_before_ts = funding_before_ts.merge(investor_count, left_on=\"uuid\", right_on=\"funding_round_uuid\", how='left', suffixes=('', 'wup'))\n",
    "\n",
    "# 3. Filter the investments for the last funding round in the Warmup window for each company\n",
    "last_round_investments = investments_warmup.groupby('funding_round_uuid').last()\n",
    "\n",
    "# 4. Calculate the number of unique investors for each company in the last funding round\n",
    "last_round_investor_count = last_round_investments.groupby('funding_round_uuid')['investor_uuid'].nunique().rename('last_round_investor_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d31859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge last round investor count into funding_rounds DataFrame\n",
    "funding_before_ts = funding_before_ts.merge(last_round_investor_count, left_on=\"uuid\", right_on=\"funding_round_uuid\", how='left', suffixes=('', '_wup'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0cd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in uuid_org: 19127\n"
     ]
    }
   ],
   "source": [
    "# Count the number of duplicates in the 'uuid_org' column\n",
    "num_duplicates = funding_before_ts['org_uuid'].duplicated().sum()\n",
    "\n",
    "# Print the number of duplicates\n",
    "print(f'Number of duplicates in uuid_org: {num_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in uuid_org: 19127\n"
     ]
    }
   ],
   "source": [
    "# Count the number of duplicates in the 'uuid_org' column\n",
    "num_duplicates = funding_before_ts['org_uuid'].duplicated().sum()\n",
    "\n",
    "# Print the number of duplicates\n",
    "print(f'Number of duplicates in uuid_org: {num_duplicates}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab21329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five duplicates:\n",
      "                                     uuid                       name  \\\n",
      "9   9ad57d08-5c54-5367-9655-d538a9e4789d       Series A - CloudBlue   \n",
      "11  6c9dd38d-f8b2-b0ee-e1ab-577b8db6238a  Seed Round - TIQ Software   \n",
      "24  ca9e1f9a-7b56-a3cf-735c-a54c50307f56     Seed Round - Cloudsnap   \n",
      "28  47d5c902-507c-82f7-c371-81f2daea007d     Seed Round - Cloudsnap   \n",
      "35  31e44b13-3f0d-567b-d900-2d7c8369a638        Angel Round - Bideo   \n",
      "\n",
      "             type                                  permalink  \\\n",
      "9   funding_round  cloudblue-technologies-series-a--9ad57d08   \n",
      "11  funding_round                tiq-software-seed--6c9dd38d   \n",
      "24  funding_round                   cloudsnap-seed--ca9e1f9a   \n",
      "28  funding_round                   cloudsnap-seed--47d5c902   \n",
      "35  funding_round                  bideo-com-angel--31e44b13   \n",
      "\n",
      "                                               cb_url      rank  \\\n",
      "9   https://www.crunchbase.com/funding_round/cloud...  296283.0   \n",
      "11  https://www.crunchbase.com/funding_round/tiq-s...   68138.0   \n",
      "24  https://www.crunchbase.com/funding_round/cloud...  316485.0   \n",
      "28  https://www.crunchbase.com/funding_round/cloud...  359204.0   \n",
      "35  https://www.crunchbase.com/funding_round/bideo...  454654.0   \n",
      "\n",
      "             created_at           updated_at country_code state_code  ...  \\\n",
      "9   2009-12-22 02:39:43  2018-02-12 23:55:45          USA         CA  ...   \n",
      "11  2010-11-03 02:18:03  2020-02-06 04:16:50          CAN         AB  ...   \n",
      "24  2012-06-13 19:42:41  2018-02-12 23:36:20          USA         TX  ...   \n",
      "28  2012-09-04 16:34:22  2018-02-12 23:40:01          USA         TX  ...   \n",
      "35  2013-02-19 00:12:29  2018-02-12 23:51:02          USA         UT  ...   \n",
      "\n",
      "   raised_amount_currency_code post_money_valuation_usd post_money_valuation  \\\n",
      "9                          USD                      NaN                  NaN   \n",
      "11                         CAD                      NaN                  NaN   \n",
      "24                         USD                      NaN                  NaN   \n",
      "28                         USD                      NaN                  NaN   \n",
      "35                         USD                1650000.0            1650000.0   \n",
      "\n",
      "   post_money_valuation_currency_code  investor_count  \\\n",
      "9                                 NaN             NaN   \n",
      "11                                NaN             1.0   \n",
      "24                                NaN             2.0   \n",
      "28                                NaN             2.0   \n",
      "35                                USD             NaN   \n",
      "\n",
      "                                org_uuid      org_name  \\\n",
      "9   fb0a896a-52d5-c318-9275-78b91fde3a23     CloudBlue   \n",
      "11  9d7c28b0-cd5a-c007-9f51-3279740b734c  TIQ Software   \n",
      "24  056cc1db-c12a-354b-24db-47b360a5a2ea     Cloudsnap   \n",
      "28  056cc1db-c12a-354b-24db-47b360a5a2ea     Cloudsnap   \n",
      "35  b26adacc-eff8-fa40-903e-cc076f31db5c         Bideo   \n",
      "\n",
      "                     lead_investor_uuids  investor_countwup  \\\n",
      "9                                    NaN                NaN   \n",
      "11  749db06b-d634-ac1b-97b8-b09ff2310557                1.0   \n",
      "24                                   NaN                2.0   \n",
      "28                                   NaN                2.0   \n",
      "35                                   NaN                NaN   \n",
      "\n",
      "   last_round_investor_count  \n",
      "9                        NaN  \n",
      "11                       1.0  \n",
      "24                       1.0  \n",
      "28                       1.0  \n",
      "35                       NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicates = funding_before_ts[funding_before_ts['org_uuid'].duplicated()]\n",
    "\n",
    "# Print only the first five rows of the duplicates DataFrame\n",
    "print(\"First five duplicates:\\n\", duplicates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e172fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_before_ts = funding_before_ts.sort_values(by=['org_uuid', 'announced_on'], ascending=[True, False])\n",
    "\n",
    "latest_funding_before_ts = funding_before_ts.drop_duplicates(subset='org_uuid', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac83e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five duplicates:\n",
      " Empty DataFrame\n",
      "Columns: [uuid, name, type, permalink, cb_url, rank, created_at, updated_at, country_code, state_code, region, city, investment_type, announced_on, raised_amount_usd, raised_amount, raised_amount_currency_code, post_money_valuation_usd, post_money_valuation, post_money_valuation_currency_code, investor_count, org_uuid, org_name, lead_investor_uuids, investor_countwup, last_round_investor_count]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicates = latest_funding_before_ts[latest_funding_before_ts['org_uuid'].duplicated()]\n",
    "\n",
    "# Print only the first five rows of the duplicates DataFrame\n",
    "print(\"First five duplicates:\\n\", duplicates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e63ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five duplicates:\n",
      " Empty DataFrame\n",
      "Columns: [uuid_org, name_org, permalink_org, domain, homepage_url, country_code, state_code, region, city, address, postal_code, status, short_description, category_list, category_groups_list, num_funding_rounds, total_funding_usd, total_funding, total_funding_currency_code, founded_on, last_funding_on, closed_on, facebook_url, linkedin_url, twitter_url, age_months, has_facebook_url, has_twitter_url, has_linkedin_url, round_count, raised_amount_usd, last_round_investment_type, last_round_raised_amount_usd, last_round_post_money_valuation, last_round_timelapse_months]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicates = unique_filtered[unique_filtered['uuid_org'].duplicated()]\n",
    "\n",
    "# Print only the first five rows of the duplicates DataFrame\n",
    "print(\"First five duplicates:\\n\", duplicates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with only the required columns\n",
    "latest_funding_before_ts = latest_funding_before_ts[['org_uuid', 'investor_countwup', 'last_round_investor_count']]\n",
    "\n",
    "# Rename 'org_uuid' to 'uuid_org' to match the column name in unique_filtered\n",
    "latest_funding_before_ts.rename(columns={'org_uuid': 'uuid_org'}, inplace=True)\n",
    "\n",
    "# Merge the DataFrames\n",
    "unique_filtered = unique_filtered.merge(latest_funding_before_ts, on='uuid_org', how='left')\n",
    "\n",
    "# Replace NaN values with 0 for companies with no data\n",
    "unique_filtered.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21141c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the unique_filtered.csv:\n",
      "uuid_org\n",
      "name_org\n",
      "permalink_org\n",
      "domain\n",
      "homepage_url\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "address\n",
      "postal_code\n",
      "status\n",
      "short_description\n",
      "category_list\n",
      "category_groups_list\n",
      "num_funding_rounds\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "last_funding_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "age_months\n",
      "has_facebook_url\n",
      "has_twitter_url\n",
      "has_linkedin_url\n",
      "round_count\n",
      "raised_amount_usd\n",
      "last_round_investment_type\n",
      "last_round_raised_amount_usd\n",
      "last_round_post_money_valuation\n",
      "last_round_timelapse_months\n",
      "investor_countwup\n",
      "last_round_investor_count\n"
     ]
    }
   ],
   "source": [
    "unique_filtered_column_names = unique_filtered.columns\n",
    "print(\"Column names in the unique_filtered.csv:\")\n",
    "for name in unique_filtered_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('unique_filtered_.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21decd3a",
   "metadata": {},
   "source": [
    "## Founders information\n",
    "The last category comprises information about the people who founded a company.\n",
    "\n",
    "In addition to the number of founder(founders_count), we synthesized new variables that provide information about the heterogeneity of the founders according to their origin number of different countries where the founders come from(founders_dif_country_count) and gender number of male (founders_male_count) and female (founders_female_count)founders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a0510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in the people.csv: 1854387\n",
      "Number of columns in the people.csv: 22\n",
      "Column names in the people.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "first_name\n",
      "last_name\n",
      "gender\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "featured_job_organization_uuid\n",
      "featured_job_organization_name\n",
      "featured_job_title\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "logo_url\n"
     ]
    }
   ],
   "source": [
    "people_path = '../Dataset/bulk_export/people.csv' \n",
    "people_data = pd.read_csv(people_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(people_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(people_data.describe())\n",
    "\n",
    "people_data_num_rows = people_data.shape[0]\n",
    "print(\"Number of people in the people.csv:\", people_data_num_rows)\n",
    "people_data_num_columns = people_data.shape[1] \n",
    "print(\"Number of columns in the people.csv:\", people_data_num_columns)\n",
    "people_data_column_names = people_data.columns\n",
    "print(\"Column names in the people.csv:\")\n",
    "for name in people_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = people_data[people_data['featured_job_organization_uuid'].isin(unique_filtered['uuid_org'])]\n",
    "\n",
    "people.to_csv('people_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b092f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Step 2: Define the regex pattern\n",
    "pattern = r'\\b(cofounder|founder|ceo|cto|cmo|cpo|chief executive|chief technology|chief operation)\\b'\n",
    "\n",
    "# Step 3: Filter the DataFrame to only include relevant rows\n",
    "filtered_people_data = people[people['featured_job_title'].apply(\n",
    "    lambda x: bool(re.search(pattern, str(x), re.IGNORECASE))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_people_data.to_csv('filtered_people_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.read_csv('people_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc87121",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered = pd.read_csv('unique_filtered_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate founders_dif_country_count, founders_male_count, and founders_female_count\n",
    "founders_info = filtered_people_data.groupby('featured_job_organization_uuid').agg(\n",
    "    founders_dif_country_count=('country_code', pd.Series.nunique),\n",
    "    founders_male_count=('gender', lambda x: (x.str.lower() == 'male').sum()),\n",
    "    founders_female_count=('gender', lambda x: (x.str.lower() == 'female').sum())\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Merge the founders_info DataFrame back into the unique_filtered DataFrame\n",
    "unique_filtered = unique_filtered.merge(founders_info, left_on='uuid_org', right_on='featured_job_organization_uuid', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for organizations with no founders information\n",
    "unique_filtered.fillna({'founders_dif_country_count': 0, 'founders_male_count': 0, 'founders_female_count': 0}, inplace=True)\n",
    "\n",
    "# Drop the redundant column after merging\n",
    "unique_filtered.drop(columns=['featured_job_organization_uuid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('unique_filtered_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d86409",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_people_data.to_csv('filtered_people_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ec0e8",
   "metadata": {},
   "source": [
    "Previous studies highlighted the importance of having a college education when building a new company. Crunchbase provides information about the education of most of the founders. However, the way this information is stored (in a free-form text) hinders the synthesis of qualitative variables about the education received by company founders.\n",
    "\n",
    "After revisiting the information contained in Crunchbase and observing that most of the education entries refer to higher education, we decided to synthesize quantitative variables about the total number of degrees obtained by company founders (founders_degree_count_total), as well as the maximum (founders_degree_count_max) and the average number of degrees (founders_degree_count_mean) among them.\n",
    "\n",
    "The sparsity problem is evident also in this category, as most of the companies do not have information about their founders or their education. In this case, we consider the absence of data as useful information and consider 0, where it corresponds. It means that the company has not updated the information about the founders in Crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a2432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of degrees in the degrees.csv: 889762\n",
      "Number of columns in the degrees.csv: 17\n",
      "Column names in the degrees.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "person_uuid\n",
      "person_name\n",
      "institution_uuid\n",
      "institution_name\n",
      "degree_type\n",
      "subject\n",
      "started_on\n",
      "completed_on\n",
      "is_completed\n"
     ]
    }
   ],
   "source": [
    "degrees_path = '../Dataset/bulk_export/degrees.csv' \n",
    "degrees_data = pd.read_csv(degrees_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(degrees_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(degrees_data.describe())\n",
    "\n",
    "degrees_data_num_rows = degrees_data.shape[0]\n",
    "print(\"Number of degrees in the degrees.csv:\", degrees_data_num_rows)\n",
    "degrees_data_num_columns = degrees_data.shape[1] \n",
    "print(\"Number of columns in the degrees.csv:\", degrees_data_num_columns)\n",
    "degrees_data_column_names = degrees_data.columns\n",
    "print(\"Column names in the degrees.csv:\")\n",
    "for name in degrees_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ddfe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_1849/1303352225.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  founders_with_degrees['degree_count'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Merge the degrees data with the filtered people data\n",
    "merged_degrees = degrees_data.merge(filtered_people_data, left_on='person_uuid', right_on='uuid', how='inner')\n",
    "\n",
    "# Step 5: Count the number of degrees for each founder\n",
    "degree_counts = merged_degrees.groupby('person_uuid').size().reset_index(name='degree_count')\n",
    "\n",
    "# Step 6: Merge the degree counts with the filtered people data to associate counts with organizations\n",
    "founders_with_degrees = filtered_people_data.merge(degree_counts, left_on='uuid', right_on='person_uuid', how='left')\n",
    "\n",
    "# Step 7: Replace NaN values in degree_count with 0\n",
    "founders_with_degrees['degree_count'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 8: Aggregate the degree counts at the organization level\n",
    "degree_stats = founders_with_degrees.groupby('featured_job_organization_uuid').agg(\n",
    "    founders_degree_count_total=('degree_count', 'sum'),\n",
    "    founders_degree_count_max=('degree_count', 'max'),\n",
    "    founders_degree_count_mean=('degree_count', 'mean')\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e16f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               uuid_org     name_org permalink_org  \\\n",
      "0  5d0cf18b-91bf-92a4-3cb6-62919c2fac08          can       onesite   \n",
      "1  fdd4bfe1-5f7b-e403-d100-91e9ec01f903    VoiceSell     voicesell   \n",
      "2  b66d4acd-65b5-46c1-effd-39d2cc41de17    TripnTale     tripntale   \n",
      "3  f5d8ef14-97a4-1179-8fbf-d66c6a3583d3  Travelgator   travelgator   \n",
      "4  68d93885-34e6-3967-786d-35b8be4df50f  SecondBrain  second-brain   \n",
      "\n",
      "             domain                 homepage_url country_code state_code  \\\n",
      "0  canarc.yooco.org  http://www.canarc.yooco.org          USA         OK   \n",
      "1     voicesell.com     http://www.voicesell.com          USA         CA   \n",
      "2     tripntale.com     http://www.tripntale.com          USA         CA   \n",
      "3   travelgator.com  https://www.travelgator.com          ROM          0   \n",
      "4    secondbrain.ai        http://secondbrain.ai          USA         TX   \n",
      "\n",
      "       region           city                                  address  ...  \\\n",
      "0    Oklahoma  Oklahoma City  14000 Quail Springs Parkway, Suite 3600  ...   \n",
      "1  California    Santa Clara                 2464 El Camino Real #209  ...   \n",
      "2  California        Alameda                                        0  ...   \n",
      "3       Sibiu          Sibiu                        Nicolaus Olahus 5  ...   \n",
      "4       Texas         Austin                            1805 e 6th st  ...   \n",
      "\n",
      "  last_round_post_money_valuation last_round_timelapse_months  \\\n",
      "0                             0.0                         0.0   \n",
      "1                             0.0                         0.0   \n",
      "2                             0.0                         0.0   \n",
      "3                             0.0                         0.0   \n",
      "4                      10000000.0                        11.0   \n",
      "\n",
      "  investor_countwup last_round_investor_count founders_dif_country_count  \\\n",
      "0               0.0                       0.0                        0.0   \n",
      "1               0.0                       0.0                        0.0   \n",
      "2               0.0                       0.0                        0.0   \n",
      "3               0.0                       0.0                        0.0   \n",
      "4               1.0                       1.0                        1.0   \n",
      "\n",
      "   founders_male_count  founders_female_count  founders_degree_count_total  \\\n",
      "0                  0.0                    0.0                          0.0   \n",
      "1                  0.0                    0.0                          0.0   \n",
      "2                  0.0                    0.0                          0.0   \n",
      "3                  0.0                    0.0                          0.0   \n",
      "4                  2.0                    0.0                          1.0   \n",
      "\n",
      "  founders_degree_count_max founders_degree_count_mean  \n",
      "0                       0.0                        0.0  \n",
      "1                       0.0                        0.0  \n",
      "2                       0.0                        0.0  \n",
      "3                       0.0                        0.0  \n",
      "4                       1.0                        0.5  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rename the column for clarity\n",
    "degree_stats.rename(columns={'featured_job_organization_uuid': 'uuid_org'}, inplace=True)\n",
    "\n",
    "# Step 9: Merge the degree statistics back into the unique_filtered DataFrame\n",
    "unique_filtered = unique_filtered.merge(degree_stats, on='uuid_org', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for companies with no education data\n",
    "unique_filtered.fillna({'founders_degree_count_total': 0, 'founders_degree_count_max': 0, 'founders_degree_count_mean': 0}, inplace=True)\n",
    "\n",
    "# Display the first 5 rows of the updated DataFrame\n",
    "print(unique_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35336760",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('unique_filtered_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be592a9c",
   "metadata": {},
   "source": [
    "# Asigning Target Variables for the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057beb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the unique_filtered_final.csv:\n",
      "uuid_org\n",
      "name_org\n",
      "permalink_org\n",
      "domain\n",
      "homepage_url\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "address\n",
      "postal_code\n",
      "status\n",
      "short_description\n",
      "category_list\n",
      "category_groups_list\n",
      "num_funding_rounds\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "last_funding_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "age_months\n",
      "has_facebook_url\n",
      "has_twitter_url\n",
      "has_linkedin_url\n",
      "round_count\n",
      "raised_amount_usd\n",
      "last_round_investment_type\n",
      "last_round_raised_amount_usd\n",
      "last_round_post_money_valuation\n",
      "last_round_timelapse_months\n",
      "investor_countwup\n",
      "last_round_investor_count\n",
      "founders_dif_country_count\n",
      "founders_male_count\n",
      "founders_female_count\n",
      "founders_degree_count_total\n",
      "founders_degree_count_max\n",
      "founders_degree_count_mean\n"
     ]
    }
   ],
   "source": [
    "organizations_data = pd.read_csv('unique_filtered_final.csv')\n",
    "\n",
    "organizations_data_column_names = organizations_data.columns\n",
    "print(\"Column names in the unique_filtered_final.csv:\")\n",
    "for name in organizations_data_column_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['operating' 'closed' 'acquired' 'ipo']\n"
     ]
    }
   ],
   "source": [
    "# Display unique values in the 'status' column\n",
    "unique_status_values = organizations_data['status'].unique()\n",
    "print(unique_status_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2c96c",
   "metadata": {},
   "source": [
    "Status has almost all the Prediction Variables that I need but now I need to change the one operating to:\n",
    "\n",
    "This way, we have defined a multi-class target variable whose value is extracted from the events occurred during the Simulation window. Using only the first event occurred for a company, we define the following classes:\n",
    "\n",
    "\n",
    "• ACQUIRED (AC): The company is acquired during the simulation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d57439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'acquired_on' to datetime format\n",
    "acquisitions_data['acquired_on'] = pd.to_datetime(acquisitions_data['acquired_on'], errors='coerce')\n",
    "\n",
    "# Merge the datasets on 'uuid_org' and 'acquiree_uuid'\n",
    "merged_data = pd.merge(organizations_data, acquisitions_data, left_on='uuid_org', right_on='acquiree_uuid', how='left', suffixes=('','_delete'))\n",
    "\n",
    "# Filter for companies acquired during the simulation window\n",
    "acquired_during_simulation = merged_data[\n",
    "    (merged_data['acquired_on'] >= simulation_start_date) & \n",
    "    (merged_data['acquired_on'] <= simulation_end_date)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e50da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'outcome' and initialize it with 'NE' (No Event)\n",
    "organizations_data['outcome'] = 'NE'\n",
    "\n",
    "# Mark acquired companies as 'AC' in the 'outcome' column\n",
    "organizations_data.loc[organizations_data['uuid_org'].isin(acquired_during_simulation['uuid_org']), 'outcome'] = 'AC'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82551d1",
   "metadata": {},
   "source": [
    "• IPO (IP): The company will go for an IPO during the simulation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'went_public_on' column to datetime\n",
    "ipos_data['went_public_on'] = pd.to_datetime(ipos_data['went_public_on'], errors='coerce')\n",
    "\n",
    "# Merge the datasets on 'uuid_org' and 'org_uuid'\n",
    "merged_ipos_data = pd.merge(organizations_data, ipos_data[['org_uuid', 'went_public_on']], left_on='uuid_org', right_on='org_uuid', how='left')\n",
    "\n",
    "# Filter for companies that went for an IPO during the simulation window\n",
    "ipo_during_simulation = merged_ipos_data[\n",
    "    (merged_ipos_data['went_public_on'] >= simulation_start_date) & \n",
    "    (merged_ipos_data['went_public_on'] <= simulation_end_date)\n",
    "]\n",
    "# Update 'outcome' column for IPO companies\n",
    "organizations_data.loc[organizations_data['uuid_org'].isin(ipo_during_simulation['uuid_org']), 'outcome'] = 'IP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ae5e5",
   "metadata": {},
   "source": [
    "• FUNDING ROUND (FR): The company reaches at least another round of funding in the simulation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd196e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'announced_on' column to datetime\n",
    "funding_rounds_data['announced_on'] = pd.to_datetime(funding_rounds_data['announced_on'], errors='coerce')\n",
    "\n",
    "# Merge the datasets on 'uuid_org' and 'org_uuid'\n",
    "merged_funding_data = pd.merge(organizations_data, funding_rounds_data[['org_uuid', 'announced_on']], left_on='uuid_org', right_on='org_uuid', how='left')\n",
    "\n",
    "# Filter for companies that had at least another round of funding during the simulation window\n",
    "funding_during_simulation = merged_funding_data[\n",
    "    (merged_funding_data['announced_on'] >= simulation_start_date) & \n",
    "    (merged_funding_data['announced_on'] <= simulation_end_date)\n",
    "]\n",
    "\n",
    "# Ensure that the outcome is not already \"IP\" or \"AC\"\n",
    "funding_during_simulation = funding_during_simulation[\n",
    "    ~funding_during_simulation['uuid_org'].isin(ipo_during_simulation['uuid_org']) &\n",
    "    ~funding_during_simulation['uuid_org'].isin(acquired_during_simulation['uuid_org'])\n",
    "]\n",
    "\n",
    "# Update 'outcome' column for funding round companies\n",
    "organizations_data.loc[organizations_data['uuid_org'].isin(funding_during_simulation['uuid_org']), 'outcome'] = 'FR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718efa85",
   "metadata": {},
   "source": [
    "• CLOSED (CL): The company is closed during the simulation window. This class is overridden by ACQUIRED when the closed and acquired events occurred simultaneously in a short period (indicating that the company was successfully acquired and then closed by the acquirer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc21fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_13808/1852381185.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  organizations_data['closed_on'] = pd.to_datetime(organizations_data['closed_on'], errors='coerce')\n",
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_13808/1852381185.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  closed_during_simulation = closed_during_simulation[\n"
     ]
    }
   ],
   "source": [
    "# Convert 'closed_on' column to datetime\n",
    "organizations_data['closed_on'] = pd.to_datetime(organizations_data['closed_on'], errors='coerce')\n",
    "\n",
    "# Filter for companies that were closed during the simulation window\n",
    "closed_during_simulation = organizations_data[\n",
    "    (organizations_data['closed_on'] >= simulation_start_date) & \n",
    "    (organizations_data['closed_on'] <= simulation_end_date)\n",
    "]\n",
    "\n",
    "# Ensure that the outcome for companies already marked as \"AC\" remains \"AC\"\n",
    "closed_during_simulation = closed_during_simulation[\n",
    "    organizations_data['outcome'] != 'AC'\n",
    "]\n",
    "\n",
    "# Update 'outcome' column for closed companies\n",
    "organizations_data.loc[organizations_data['uuid_org'].isin(closed_during_simulation['uuid_org']), 'outcome'] = 'CL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62d6ea",
   "metadata": {},
   "source": [
    "• NO EVENT (NE): None of the previous events occurred during the simulation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_data.to_csv('unique_filtered_final_with_target_variable.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aef1a5",
   "metadata": {},
   "source": [
    "Get the ratio and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Class  Frequency     Ratio\n",
      "     NO EVENT (NE)     333877 87.444247\n",
      "FUNDING ROUND (FR)      38176  9.998507\n",
      "     ACQUIRED (AC)       6810  1.783577\n",
      "       CLOSED (CL)       2245  0.587978\n",
      "          IPO (IP)        709  0.185691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "organizations_data = pd.read_csv('unique_filtered_final_with_target_variable.csv')\n",
    "\n",
    "# Calculate the frequency of each class in the 'outcome' column\n",
    "class_distribution = organizations_data['outcome'].value_counts().reset_index()\n",
    "class_distribution.columns = ['Class', 'Frequency']\n",
    "\n",
    "# Calculate the ratio of each class\n",
    "total_count = len(organizations_data)\n",
    "class_distribution['Ratio'] = (class_distribution['Frequency'] / total_count) * 100\n",
    "\n",
    "# Map the class codes to their descriptions\n",
    "class_mapping = {\n",
    "    'CL': 'CLOSED (CL)',\n",
    "    'AC': 'ACQUIRED (AC)',\n",
    "    'FR': 'FUNDING ROUND (FR)',\n",
    "    'IP': 'IPO (IP)',\n",
    "    'NE': 'NO EVENT (NE)'\n",
    "}\n",
    "\n",
    "class_distribution['Class'] = class_distribution['Class'].map(class_mapping)\n",
    "\n",
    "# Display the results\n",
    "print(class_distribution.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe43c7",
   "metadata": {},
   "source": [
    "No Event class is too hight and looking and the data there is lots of companies that their current status is closed but the outcome is NE. That means that they were closed after the simulation window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1997af8",
   "metadata": {
    "tags": [
     "Models part"
    ]
   },
   "source": [
    "# Models and Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e9b5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('unique_filtered_final_with_target_variable.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e68687c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the data.csv:\n",
      "uuid_org\n",
      "name_org\n",
      "permalink_org\n",
      "domain\n",
      "homepage_url\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "address\n",
      "postal_code\n",
      "status\n",
      "short_description\n",
      "category_list\n",
      "category_groups_list\n",
      "num_funding_rounds\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "last_funding_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "age_months\n",
      "has_facebook_url\n",
      "has_twitter_url\n",
      "has_linkedin_url\n",
      "round_count\n",
      "raised_amount_usd\n",
      "last_round_investment_type\n",
      "last_round_raised_amount_usd\n",
      "last_round_post_money_valuation\n",
      "last_round_timelapse_months\n",
      "investor_countwup\n",
      "last_round_investor_count\n",
      "founders_dif_country_count\n",
      "founders_male_count\n",
      "founders_female_count\n",
      "founders_degree_count_total\n",
      "founders_degree_count_max\n",
      "founders_degree_count_mean\n",
      "outcome\n"
     ]
    }
   ],
   "source": [
    "data_column_names = data.columns\n",
    "print(\"Column names in the data.csv:\")\n",
    "for name in data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad479",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "We have considered five different machine learning classifier algorithms: \n",
    "\n",
    "- Support Vector Machines (SVM),\n",
    "\n",
    "### Tree-based ensemble classifiers \n",
    "- Decision Trees (DT),\n",
    "- Random Forests (RF), \n",
    "- Extremely Randomized Trees (ERT)\n",
    "- Gradient Tree Boosting (GTB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dec998e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome variable names: ['AC' 'CL' 'FR' 'IP' 'NE']\n",
      "Features included in training:\n",
      "['country_code', 'state_code', 'region', 'city', 'status', 'category_list', 'category_groups_list', 'num_funding_rounds', 'total_funding_usd', 'total_funding', 'age_months', 'has_facebook_url', 'has_twitter_url', 'has_linkedin_url', 'round_count', 'raised_amount_usd', 'last_round_investment_type', 'last_round_raised_amount_usd', 'last_round_post_money_valuation', 'last_round_timelapse_months', 'investor_countwup', 'last_round_investor_count', 'founders_dif_country_count', 'founders_male_count', 'founders_female_count', 'founders_degree_count_total', 'founders_degree_count_max', 'founders_degree_count_mean']\n",
      "\n",
      "Features excluded from training:\n",
      "['uuid_org', 'name_org', 'permalink_org', 'domain', 'homepage_url', 'address', 'postal_code', 'short_description', 'facebook_url', 'linkedin_url', 'twitter_url', 'founded_on', 'last_funding_on', 'closed_on', 'total_funding_currency_code', 'outcome']\n",
      "IPO_vs_Other: Mean precision = 0.8137, Std = 0.0317\n",
      "IPO_vs_Other: Mean recall = 0.7433, Std = 0.0432\n",
      "FR_vs_Other: Mean precision = 0.9101, Std = 0.0030\n",
      "FR_vs_Other: Mean recall = 0.9713, Std = 0.0010\n",
      "AC_vs_Other: Mean precision = 0.7982, Std = 0.0095\n",
      "AC_vs_Other: Mean recall = 0.8667, Std = 0.0082\n",
      "Training and evaluation results have been saved to 'model_training_results.pkl'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pickle\n",
    "\n",
    "# Assuming data is already loaded in 'data' DataFrame\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = [\n",
    "    'country_code', 'state_code', 'region', 'city', 'status', \n",
    "    'category_list', 'category_groups_list', 'last_round_investment_type'\n",
    "]\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "# Encode target variable\n",
    "data['outcome'] = le.fit_transform(data['outcome'].astype(str))\n",
    "\n",
    "# Print all outcome variable names\n",
    "outcome_names = le.classes_\n",
    "print(\"Outcome variable names:\", outcome_names)\n",
    "\n",
    "# Define features and print included and excluded features\n",
    "excluded_features = [\n",
    "    'uuid_org', 'name_org', 'permalink_org', 'domain', 'homepage_url', \n",
    "    'address', 'postal_code', 'short_description', 'facebook_url', \n",
    "    'linkedin_url', 'twitter_url', 'founded_on', 'last_funding_on', \n",
    "    'closed_on', 'total_funding_currency_code', 'outcome'\n",
    "]\n",
    "X = data.drop(columns=excluded_features)\n",
    "\n",
    "print(\"Features included in training:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "print(\"\\nFeatures excluded from training:\")\n",
    "print(excluded_features)\n",
    "\n",
    "# Binary targets for specified classifications\n",
    "data['IPO_vs_Other'] = (data['outcome'] == le.transform(['IP'])[0]).astype(int)\n",
    "data['FR_vs_Other'] = (data['outcome'] == le.transform(['FR'])[0]).astype(int)\n",
    "data['AC_vs_Other'] = (data['outcome'] == le.transform(['AC'])[0]).astype(int)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'IPO_vs_Other': RandomForestClassifier(),\n",
    "    'FR_vs_Other': RandomForestClassifier(),\n",
    "    'AC_vs_Other': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "positive_predictions = {'IPO_vs_Other': [], 'FR_vs_Other': [], 'AC_vs_Other': []}\n",
    "\n",
    "for target, clf in classifiers.items():\n",
    "    y = data[target]\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        precision_scores.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        recall_scores.append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        # Identify true positive predictions\n",
    "        for i in range(len(y_test)):\n",
    "            if y_pred[i] == 1 and y_test.iloc[i] == y_pred[i]:\n",
    "                positive_predictions[target].append({\n",
    "                    'index': X_test.index[i]\n",
    "                })\n",
    "    \n",
    "    results[target] = {\n",
    "        'mean_precision': np.mean(precision_scores),\n",
    "        'std_precision': np.std(precision_scores),\n",
    "        'mean_recall': np.mean(recall_scores),\n",
    "        'std_recall': np.std(recall_scores)\n",
    "    }\n",
    "    \n",
    "    # Train on the full dataset for predictions\n",
    "    clf.fit(X, y)\n",
    "    data[f'{target}_Prediction'] = clf.predict(X)\n",
    "\n",
    "    print(f\"{target}: Mean precision = {np.mean(precision_scores):.4f}, Std = {np.std(precision_scores):.4f}\")\n",
    "    print(f\"{target}: Mean recall = {np.mean(recall_scores):.4f}, Std = {np.std(recall_scores):.4f}\")\n",
    "\n",
    "# Save training and evaluation results to a file\n",
    "with open('model_training_results.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'results': results,\n",
    "        'positive_predictions': positive_predictions\n",
    "    }, file)\n",
    "\n",
    "print(\"Training and evaluation results have been saved to 'model_training_results.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2450c085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive predictions have been saved to 'positive_predictions.md'\n",
      "Results and variables have been saved to 'model_results.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the results from the training phase\n",
    "with open('model_training_results.pkl', 'rb') as file:\n",
    "    data_dict = pickle.load(file)\n",
    "\n",
    "results = data_dict['results']\n",
    "positive_predictions = data_dict['positive_predictions']\n",
    "\n",
    "# Load the original data (assuming it is still available)\n",
    "# data = pd.read_csv('path_to_data.csv')  # Uncomment and provide the path if necessary\n",
    "\n",
    "# Extract positive predictions with company data\n",
    "ipo_predictions = pd.DataFrame(positive_predictions['IPO_vs_Other'])\n",
    "fr_predictions = pd.DataFrame(positive_predictions['FR_vs_Other'])\n",
    "ac_predictions = pd.DataFrame(positive_predictions['AC_vs_Other'])\n",
    "\n",
    "if not ipo_predictions.empty:\n",
    "    ipo_predictions = ipo_predictions.merge(data, left_on='index', right_index=True)\n",
    "if not fr_predictions.empty:\n",
    "    fr_predictions = fr_predictions.merge(data, left_on='index', right_index=True)\n",
    "if not ac_predictions.empty:\n",
    "    ac_predictions = ac_predictions.merge(data, left_on='index', right_index=True)\n",
    "\n",
    "# Select a sample of 15 from each\n",
    "ipo_sample = ipo_predictions.sample(n=15, random_state=42) if len(ipo_predictions) > 15 else ipo_predictions\n",
    "fr_sample = fr_predictions.sample(n=15, random_state=42) if len(fr_predictions) > 15 else fr_predictions\n",
    "ac_sample = ac_predictions.sample(n=15, random_state=42) if len(ac_predictions) > 15 else ac_predictions\n",
    "\n",
    "# Formatting the results for better readability\n",
    "def format_predictions(df, title):\n",
    "    formatted = f\"# {title}\\n\\n\"\n",
    "    for idx, row in df.iterrows():\n",
    "        formatted += f\"## Company {idx + 1}\\n\\n\"\n",
    "        for col in df.columns:\n",
    "            if col != 'index':\n",
    "                formatted += f\"**{col}:** {row[col]}\\n\\n\"\n",
    "        formatted += \"\\n\\n\"\n",
    "    return formatted\n",
    "\n",
    "ipo_formatted = format_predictions(ipo_sample, \"IPO Predictions\")\n",
    "fr_formatted = format_predictions(fr_sample, \"Funding Round Predictions\")\n",
    "ac_formatted = format_predictions(ac_sample, \"Acquisition Predictions\")\n",
    "\n",
    "# Save formatted results to a markdown file\n",
    "with open('positive_predictions.md', 'w') as file:\n",
    "    file.write(ipo_formatted)\n",
    "    file.write(\"\\n\\n\")\n",
    "    file.write(fr_formatted)\n",
    "    file.write(\"\\n\\n\")\n",
    "    file.write(ac_formatted)\n",
    "\n",
    "print(\"Positive predictions have been saved to 'positive_predictions.md'\")\n",
    "\n",
    "# Save the results and positive predictions to a file using pickle\n",
    "with open('model_results.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'results': results,\n",
    "        'positive_predictions': pd.concat([ipo_sample, fr_sample, ac_sample])\n",
    "    }, file)\n",
    "\n",
    "print(\"Results and variables have been saved to 'model_results.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f39cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_results.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "results = data['results']\n",
    "outcome_distribution = data['outcome_distribution']\n",
    "classes = data['classes']\n",
    "\n",
    "# Print saved results\n",
    "for clf_name, result in results.items():\n",
    "    print(f\"{clf_name}: Mean accuracy = {result['mean_accuracy']:.4f}, Std = {result['std_accuracy']:.4f}\")\n",
    "\n",
    "# Print saved outcome distribution\n",
    "print(\"\\nClass value distribution of the outcome variable:\")\n",
    "print(\"Class\\tFrequency\\tRatio\")\n",
    "for cls, stats in outcome_distribution.items():\n",
    "    print(f\"{cls}\\t{stats['count']}\\t{stats['ratio']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
